# Learning to Prompt for Continual Learning
## 术语与翻译
### L2P
Learning to Prompt
### CLS:Complementary Learning Systems
解释了人脑中两个不同记忆系统之间的互补作用，尤其是海马体与新皮层之间的合作关系。
#### 海马体
负责快速学习和短期记忆。它可以迅速形成新的记忆，尤其是关于具体事件或经历的记忆（情景记忆，episodic memory）。然而，这种快速学习容易导致干扰或遗忘，因此这种记忆系统不适合长期存储大量信息。
#### 新皮层
负责逐渐整合记忆，将信息转化为长期记忆。这一过程缓慢但允许大规模存储并避免记忆干扰。
## 0.摘要
主流的持续学习（continual learning）范式的核心挑战是适应非平稳的数据分布，并防止灾难性遗忘（catastrophic forgetting）。通常的方法是在测试时使用先前的任务缓冲区或已知的任务身份来检索先前学到的知识，从而避免遗忘。而本文提出了一种新的持续学习范式，其目标是在不访问任务身份的情况下，训练出更简洁的模型系统。我们的方法是动态地提示（L2P）预训练模型，使其能够在不同的任务转换下学习任务。在我们提出的框架中，提示是可以学习的参数，它们被维护在一个记忆空间中。我们的目标是优化这些提示，以指导模型预测并显式管理任务不变和任务特定的知识，同时保持模型的可塑性。我们在多种图像分类基准上进行了全面的实验，结果表明，L2P在各种具有挑战性的持续学习设置中持续优于现有的最先进方法。令人惊讶的是，即使没有使用任务缓冲区，L2P也在没有任务缓冲的情况下取得了优异的结果，直接适用于具有挑战性的任务不可知持续学习。

## introduction
与独立同分布（i.i.d.）数据上的普通监督学习相反，continual learning处理的是在non-sationary数据分布上训练单个模型的问题，其中不同的分类任务是按顺序出现的。然而，由于模型在学习过程中每个阶段仅能访问当前的数据，它容易对当前可用的数据过拟合，并且会在先前训练数据上的性能下降，即灾难性遗忘。

持续学习的主要研究工作遵循一种学习范式，通过在数据分布变化时不断调整整个或部分模型权重，重点是保持过去的知识。尽管许多类型的方法取得了良好的结果，仍有一些关键的限制需要解决。

首先，根据互补学习系统（CLS）理论，受海马体中的情景记忆的启发，许多最先进的方法依赖于一个重演缓冲区，以重新训练部分过去的样本。然而，当缓冲区大小较小时，它们的性能会显著下降，并且在不允许使用重演缓冲区的情况下（如数据隐私重要的现实场景），它们变得无效。这表明，简单地缓冲过去的数据并重新训练模型可能不是恢复过去知识的最佳方法。在不访问重演缓冲区的情况下，另一些研究通过假设测试时已知任务身份，绕过了遗忘问题，从而能够将任务无关的模块附加到共享模型上进行推理。然而，在测试时要求知道任务身份限制了其实际应用。前期工作的局限性提出了在持续学习中的关键问题：(1) 情景记忆的形式能否超越缓冲过去的数据，成为更智能和简洁的情景记忆系统？(2) 如何在不知道任务身份的情况下，自动选择相关的知识组件来应对任意样本？

对于第一个问题，我们从最近基于提示的学习（提示法）中获得灵感，这是NLP领域中一种新的迁移学习技术。Prompting通过设计包含任务特定信息的模板化或可学习的提示标记，来调整模型的文本输入，从而使预训练的语言模型能够处理参数化输入并执行与提示相关的特定预测任务。直观上，基于提示的学习将学习下游任务从直接调整模型权重转变为设计提示，"指示"模型有条件地执行任务。一个提示编码了任务特定的知识，并且比普通的微调更有效地利用预训练的冻结模型。因此，在持续学习的背景下，利用提示来学习知识并进一步存储已学知识是很有前途的。

对于第二个问题，prompting技术还没有好的处理方式。一方面，如果我们在持续学习中为不同任务训练不同的提示，在测试时仍需要知道任务身份才能使用适当的任务特定提示进行预测；另一方面，作为一种迁移学习技术，提示法的目标是使冻结的预训练模型在单个下游任务中表现良好，而不是在任务序列中。因此，如果我们为所有任务维护一个共享提示，灾难性遗忘的问题可能仍然存在。

为此，我们提出了一种新的持续学习方法，称为基于提示的持续学习（Learning to Prompt for Continual Learning，L2P）。该方法与流行的基于重演的学习方法（rehearsal-based methods）相互独立，并且适用于在未知任务身份或边界的实际持续学习场景中。图1展示了我们方法与典型持续学习方法的对比概述。L2P利用预训练模型中的代表性特征；然而，与在持续学习过程中微调参数不同，L2P保持预训练模型不变，而是学习一组提示，这些提示动态地指导模型解决相应的任务。具体来说，这些提示被组织在一个称为提示池（prompt pool）的键值共享记忆空间中，我们设计了一种查询机制，能够根据实例级的输入特征动态查找与任务相关的一部分提示。提示池与监督损失联合优化，确保共享提示编码共享知识以实现知识迁移，而非共享提示则编码任务特定的知识，帮助保持模型的可塑性。我们的设计显式地分离了共享知识和任务特定知识，从而在优化过程中大幅减少了任务特定知识之间的干扰，进而实现了在不需要重演缓冲区的情况下，最小化灾难性遗忘。实例级的查询机制消除了识别任务身份或边界的必要性，使得在最具挑战性但研究较少的任务无关持续学习中成为可能。然后，将所选的提示添加到输入嵌入（图2）之前，这些提示隐含地为预训练模型添加了任务相关的指令，使模型能够回忆出执行相应任务所需的最相关特征。

综上所述，这项工作有以下贡献。

1. 我们提出了L2P，一个基于提示持续学习的新型持续学习框架，提供了一种新的机制，通过学习提示池记忆空间来解决持续学习的挑战，这些提示作为参数化的 "指令"，供预训练的模型按顺序学习任务。该方法适用于处理最具挑战性的任务无关的持续学习。

2. 我们进行了全面的实验，证明了L2P在多个持续学习基准上的有效性，包括类和域的递增以及任务无关的设置。我们所提出的L2P在所有基准上都超过了以前的最先进的方法。令人惊讶的是，即使不使用排练缓冲区，L2P仍然取得了与基于排练的方法相竞争的结果，这在禁止排练缓冲区的现实世界场景中是非常理想的。

3. 据我们所知，我们是第一个在持续学习领域引入提示思想的人。我们期望我们的方法为解决持续学习中的前沿挑战提供一个不同的视角。

## Related Work
### Continual Learning
最近的持续学习算法主要有三类。基于正则化的方法[1, 21, 28, 65]通过限制对以前任务的重要参数的学习率来限制模型的可塑性。尽管这些方法在一定程度上解决了灾难性遗忘的问题，而没有存储过去的例子，但在具有挑战性的设置[34]或复杂的数据集[49, 61]下，它们不能获得令人满意的性能。

#### Rehearsal-based methods
基于排练的方法[7, 8, 17]构建了一个数据缓冲区，以保存旧任务的样本，用当前任务的数据进行训练。基于这个简单而有效的想法，许多最近的方法通过涉及额外的知识蒸馏惩罚[3, 6, 49, 61]来改进它。或利用自监督的学习技术[4, 44]。尽管它的概念很简单，但基于排练的方法在各种基准上取得了最先进的性能[34, 42]。然而，基于排练的方法的性能通常会随着缓冲区大小的减少而恶化[4]，并且基于排练的方法最终不适用于需要考虑数据隐私的场景[54]。与直接保存过去的知识数据来重新训练模型不同，我们的方法将过去的知识存储在小的可学习的提示参数中，以提示模型处理当前的任务，并反过来将当前的知识累积到提示中。我们的方法不需要排练缓冲区就能达到与基于排练的方法相近的性能，并且可以进一步改进，以便在给定一个小的排练缓冲区时设定一个新的技术状态。

#### Architecture-based methods
基于架构的方法旨在为每个任务提供单独的组件。可以通过扩展网络[26, 31, 48, 50, 64, 68]来识别特定任务的组件，或者参加特定任务的子网络[19,35, 51,59]。然而，大多数方法都需要在测试时以任务身份作为网络的条件，当任务身份未知时，这些方法并不适用于更现实的类增量和任务无关性设置。最近的一些方法要么直接推断出任务身份[60]，要么额外增加排练缓冲区以绕过这个问题[44, 63]。然而，这些方法需要大量的额外参数，有时接近于完整模型的大小[19,59]。相反，L2P不需要测试阶段的任务识别，只增加了可忽略不计的额外参数（0.1%）。虽然L2P也引入了额外的提示参数，但它的设计原理与基于架构的方法完全不同。L2P设计了一个新颖的基于提示的存储器，从模型输入中学习高级指令来引导模型输出，并保持学习的架构固定。相比之下，大多数基于架构的方法旨在分离模型参数。
最后，最近CTN[45]和DualNet[44]的工作开始考虑通过一个控制器进行知识管理，该控制器除了主干模型外，还对任务级信息进行建模。然而，CTN在测试时仍然需要任务身份，而DualNet需要一个排练缓冲区才能工作。此外，CTN和DualNet的灵感来自CLS的不同角度，它认为人类是通过两个系统分别促进快速学习和长期记忆来实现持续学习的。有趣的是，尽管我们得到的灵感不同，但L2P完全可以通过CLS理论进行解释。提示池处理的是快速学习，而骨干模型则是长期记忆。

### Prompting for transfer learning
迁移学习中的提示。提示的高层次想法是应用一个函数来修改输入的文本，从而使语言模型获得关于任务的额外信息。然而，提示功能的设计是具有挑战性的，需要启发式方法。最近的工作，包括prompt tuning[25]和prefix tuning[27]，试图通过在一个连续的空间中应用可学习的提示来解决这个问题，在迁移学习上取得优异的表现。与其竞争对手，如Adapter[43, 58]和LoRA[18]相比，提示以更小的额外参数捕获特定任务的知识。提示的中心思想主要是为迁移学习设计的。请注意，在持续学习中直接应用提示是不可行的。我们提出的新型框架揭示了其对持续学习问题的价值。

## 3. Prerequisites
### 3.1 Continual learning protocols

持续学习通常被定义为在连续任务的非平稳数据上训练机器学习模型。我们定义一个任务序列$\mathcal{D} = \{D_1, \dots, D_T\}$，其中第 i 个任务$D_t = \{(x_i^t, y_i^t)\}_{i=1}^{n_t}$包含输入样本$x_i^t \in \mathcal{X}$和其对应的标注$y_i^t \in \mathcal{Y}$。目标训练一个单一的模型$f : \mathcal{X} \to \mathcal{Y}$，参数为$\theta$，这样它能预测标注$y = f_\theta(x) \in \mathcal{Y}$，给出任务中未见的测试样本$x$。在训练末来的任务时，可能不会再看到以前任务的数据。

根据任务转移环境的不同，持续学习可以分为多种设置，挑战也略有不同。常见的任务--、类--和域--增量设置\ 假定任务数据$D_t$以离散的方式按 t = \{1, \dots, T\} 的顺序到达。

- 任务增量学习假设任务身份在测试时是已知的，通常被认为是最简单的设置[34, 38]。
- 领域递增学习为每个任务保持相同的类别集，只改变任务中的分布。
- 在更具挑战性的任务无关设置中，$D_t$中的任务数据不稳定，而任务身份$t$是未知的。

我们的论文解决了更具挑战性的类增量和域增量**问题**，并进一步探索了任务无关的设置。

### 3.2 Prompt-based learning and baselines

基于提示的学习是NLP中的一种新兴技术。与传统的监督式微调不同，这类方法设计了特定任务的**提示函数**，以指示预测的模型有条件地执行相应的任务[29]。最近的技术之一，Prompt Tuning (PT) [25]，提出了简单的条件任务标的T5-like语言模型[47]，通过学习提示参数来执行下游的NLP任务，这些提示参数被加到输入标记中来指示模型的预测。在不变学习通用的情况下，这里我们使用基于图像模态transformer+的系列模型[10, 56]来介绍PT的定义。这个定义很容易被推广到其他模态并基于序列的模型。

给出一个二维图像 $x \in \mathbb{R}^{H \times W \times C}$ 的输入和一个预训练的视觉Transformer (ViT) $f = f_r \circ f_e$（不包括分类头**），其中 $f_e$ 是输入嵌入层， $f_r$ 代表一堆自注意层[10]。图像被重塑为一串扁平化的二维patches $x_p = [P_e ; e]$，其中 $L$ 是标记长度，即patch的数量，$S$ 是patch大小，$C$ 是原始通道的数量。为了简化符号，我们假设 $x_p$ 中的第一个标记是作为预训练模型[10]的一部分的类标记。预训练的嵌入层 $f_e : \mathbb{R}^{L \times S^2 \cdot C} \to \mathbb{R}^{L \times D}$ 将任务补后的图像映射到嵌入特征空间 $e = f_e(x) \in \mathbb{R}^{L \times D}$，其中 $D$ 是嵌入维度。在解决多个下游任务时，我们保持大规模的预训练骨干的结构，以保持PT之后的通用性。PT的直接应用是将可学习的参数 $P_e \in \mathbb{R}^{L_p \times D}$ 称为提示，预置到嵌入特征 $x_p = [P_e ; e]$，并将扩展序列输入模型函数 $f_r(x_p)$ 用于执行分类任务。不同的任务有独立的提示，并共享一个大模型的副本。

与普通的微调相比，文献显示，**基于提示的学习会使基于序列的模型具有更高的学习特征的能力**[25,29]。尽管它在迁移学习中成功地为每个任务训练了单独的提示，但提示方法不能直接应用于测试时任务身份未知的持续学习场景中。

## 4.Learning to Prompt (L2P)
### From prompt to prompt pool

引入提示池的动机有三个方面。首先，测试时的任务身份是未知的，所以训练与任务无关的提示是不可行的。第二，即使任务无关的提示在测试时可以知道，它也会阻止类似任务之间可能的知识共享[16]。第三，虽然为所有任务学习一个单一的共享提示的天真方式能够实现知识共享，但它仍然会引发严重的意向问题（见第4.5节）。理想情况下，**我们会学习一个在任务相似时能够共享知识的模型，而在其他情况下保持知识的独立性**。因此，我们建议使用一个提示池来存储编码的知识，这些知识可以被灵活地组合作为模型的输入。提示池被定义为

$P = \{P_1, P_2, \dots, P_M\}, M = \text{total of prompts}$

其中 $P_j \in \mathbb{R}^{L_p \times D}$ 是一个具有标记长度 $L_p$ 和与 $x_e$ 相同的嵌入大小 $D$ 的单一提示。$x$ 和 $x_e = f_e(x)$ 分别为输入和其相应的嵌入特征。注意，在我们的符号中，我们省略了 $x$ 的任务索引 $t$，因为我们的方法对于任务无关的设置是足够通用的。用 $\{s_i\}_{i=1}^{N}$ 表示来自 $[1, M]$ 的 $N$ 个指数的子集，然后我们可以将输入嵌入进行如下调整。

$x_p = [P_{s_1}, \dots, P_{s_N}; x_e], 1 \leq N \leq M$

其中，`;` 代表标记token长度维度的连接。提示语句可以自由组合，因此它们可以**编码知识**（例如知识特征或任务信号）供模型处理。理想情况下，我们希望通过灵活的提示组合实现更精细的知识共享：类似的输入会在共享更多的共同提示，反之亦然。

### 4.2 Instance-wise prompt query

我们设计了一个基于键值对的查询策略，为不同的输入动态地选择合适的提示（见图2）。这种键值记忆查询机制与其他领域的方法共享一些设计原则，如可微分神经计算机[14]和VQ-VAE[41]，它们有外部记忆需要维护，并根据不同的目的来使用。

我们将每个提示值与一个可学习的键相关联：$\{(k_1, P_1), (k_2, P_2), \dots, (k_M, P_M)\}$，其中 $k_i \in \mathbb{R}^D$。而我们用 $K = \{k_i\}_{i=1}^M$ 表示所有键的集合。理想情况下，我们希望输入 $x$ 实例本身通过查询-键匹配来决定选择哪些提示。为此，我们引入一个查询函数 $q : \mathbb{R}^{H \times W \times C} \to \mathbb{R}^D$，将输入 $x$ 的编码为与键相同的维度。此外，对于不同的任务，$q$ 应该是一个确定性的函数，并且没有可学习的参数。我们假设使用预训练模型作为该查询的**特征提取**器来获得高阶特征 $q(x) = f_\theta(x)[0,:]$（我们用来对应于 [class] 的特征向量）。其他特征提取器如ConvNet也同样适用。

我们用 $\gamma : \mathbb{R}^D \times \mathbb{R}^D \to \mathbb{R}$ 是一个对查询和提示键之间的匹配进行评分的函数（我们发现点积距离效果不错）。给定一个输入 $x$，我们用 $q(x)$ 通过简单的求解目标来查找前N个键。

$
K_x = \arg\min_{\{s_i\}_{i=1}^N \subseteq [1, M]} \sum_{i=1}^{N} \gamma(q(x), k_{s_i}), \text{equation}(3)
$

请注意，这种键值策略的设计使查询机制的学习和提示的学习过程脱钩，这一点已经被实验证明是至关重要的（见第4.5节）。此外，查询提示是以实例的方式进行的，这使得提示框架具有任务无关性，意味着该方法在训练结束时不需要明确的任务边界，在测试时也不需要任务身份。

可选择多样化的提示示例。虽然我们的方法不需要任务边界信息，但在现实世界的场景和实验数据**集中**，任务身份是离散的，所以任务边界训练也是已知的，这是非常普通的。我们发现，在我们的方法中加入这种提示选择，可以帮助模型学习更好的特定任务提示，尤其是在任务具有高度多样性的情况下。为此，我们提出了一个简单的扩展，**增加任务身份实验**，这对L2P来说是可选的。

在任务 t 的训练过程中，我们维护一个提示调速率 $H_t = [h_1, h_2, \dots, h_m]$，其中每个项 $h_i$ 代表到任务 t 的几项上。提示 $P_i$ 被选择的标记化加权输出。为了鼓励查询机制选择不同的提示，我们将公式修改为

$
K_x = \arg\min_{\{s_i\}_{i=1}^N \subseteq [1, M]} \sum_{i=1}^{N} \gamma(q(x), k_{s_i}) \cdot h_{s_i}
$
其中 $h_{s_i}$对被选中的经常使用的提示语进行惩罚，以鼓励多样化的选择。方程4只在训练时适用；在测试时，使用方程3。

### 4.3 Optimization objective for L2P

在每一个训练步骤中，在按照上述查询策略选择 $N$ 个提示后，适应的嵌入特征 $x_p$ 被送入预训练模型 $f_r$ 的其余部分和由 $\phi$ 参数化的最终分类器 $g_\phi$。总的来说，我们寻求使端到端的训练损失**函数**最小化。

$
\min_{P, K, \phi} L(g_\phi(f_r^{avg}(x_p)), y) + \lambda \sum_{K_x} \gamma(q(x), k_{s_i}), \text{equation}(5)
$

其中，$K_x$ is obtained with equation 3,

其中 $f_r^{avg} = AvgPool(f_r(x_p)[0:N \cdot L_p,:])$，即在分类头前对 $N \cdot L_p$ 提示位置对应的输出隐藏向量进行平均化。第一个项是softmax交叉熵损失，第二个项是surrogate损失，目的是将选定的键拉近到相应的查询特征。 $\lambda$ 是一个标量，用于加权损失。
