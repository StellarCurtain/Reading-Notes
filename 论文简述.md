# Open-environment Machine Learning
## 特征改变 (feature invariant)：

## 学习目标改变 (learning objective invariant)：


## 数据分布改变 (data distribution invariant)：

## 类别标签改变 (class label invariant)：
新兴类别学习
识别-容纳

# LoRA
我们假设在模型适应的过程中，权重的更新也具有低的“内在秩”。对于一个预训练的权重矩阵 $W_0 \in R^{d \times k}$，我们通过将其更新表示为低秩分解来约束它：$W_0 + \Delta W = W_0 + BA$，其中 $B \in R^{d \times r}$，$A \in R^{r \times k}$，并且秩 $r \ll \min(d, k)$。在训练过程中，$W_0$被冻结，不接收梯度更新，而 $A$ 和 $B$ 包含可训练参数。请注意，$W_0$ 和 $\Delta W = BA$ 都与相同的输入相乘，且其各自的输出向量逐坐标相加。对于 $h = W_0x$，我们修改后的前向传播公式为：

$h = W_0x + \Delta W x = W_0x + BAx$

其中对 $A$ 使用随机高斯初始化，对 $B$ 使用零初始化，因此训练开始时 $\Delta W = BA$ 为零。然后我们通过因子 $\alpha / r$ 对 $\Delta W x$ 进行缩放，其中 $\alpha$ 是 $r$ 中的一个常数。当使用Adam优化时，如果我们适当缩放初始化，那么调整 $\alpha$ 大致等同于调整学习率。因此，我们简单地将 $\alpha$ 设置为我们尝试的第一个 $r$，而无需进一步调整它。

## 实际的好处与限制
最显著的好处是内存和存储使用量的减少。

另一个好处是，在实际部署中，我们可以显式计算并存储 $W = W_0 + BA$，并像往常一样进行推理。请注意，$W_0$ 和 $BA$ 都位于 $R^{d \times k}$ 中。当我们需要切换到另一个下游任务时，我们可以通过减去 $BA$ 来恢复 $W_0$，然后添加不同的 $B_0A_0$，从而以更低的成本在任务之间切换。这允许我们在存储了预训练权重的机器上即时切换多个定制模型。

LoRA也有其局限性。例如，如果选择将 $A$ 和 $B$ 吸收到 $W$ 中以消除额外的推理延迟，那么很难在单次前向传播中对不同任务的输入进行批处理。然而，在不要求低延迟的场景中，可以不合并权重，并为批次中的样本动态选择要使用的LoRA模块。

# QLoRA
QLORA引入了多项创新来节省内存而不牺牲性能：
(a) 4位NormalFloat (NF4)，一种对于正态分布权重信息理论上最优的数据类型；


(b) 双重量化，通过量化量化常数来减少平均内存占用；
量化是将输入从具有更多信息的表示离散化为具有较少信息的表示的过程。它通常意味着将数据类型减少为更少的位数，例如将32位浮点数转换为8位整数。为了确保低位数据类型的整个范围都被使用，输入数据类型通常通过归一化到目标数据范围来重新缩放。例如，将32位浮点（FP32）张量量化为范围为 [−127,127]的Int8张量
$X^{\text{Int8}} = \text{round}\left( \frac{127}{\text{absmax}(X^{\text{FP32}})} \times X^{\text{FP32}} \right) = \text{round}(c^{\text{FP32}} \cdot X^{\text{FP32}}),$ 𝑐是量化常数或量化尺度。

反量化公式:$\text{dequant}(c^{\text{FP32}}, X^{\text{Int8}}) = \frac{X^{\text{Int8}}}{c^{\text{FP32}}} = X^{\text{FP32}}$

这个方法的问题在于，如果输入张量中出现较大数值（即离群值），那么量化区间中的某些位组合可能不会被有效利用，导致一些数值被量化到很少的区间中。为了解决这个离群值问题，通常的方法是将输入张量划分为独立的块，每个块都有自己的量化常数c。这可以形式化为：我们将输入张量 
$X∈R^{b×h}$切割为大小为B的连续块，通过展平输入张量并将线性段分割为 
$n= (b×h)/B​$个块。我们独立地量化这些块，并使用公式1来创建量化的张量和 
n个量化常数$c_i$。

我们引入了 **双重量化 (Double Quantization, DQ)**，即量化量化常数，以进一步节省内存。虽然精确的 4 位量化需要较小的块大小，但它也带来了相当大的内存开销。例如，使用32位常数和64大小的块时，量化常数平均每个参数会增加0.5位。双重量化有助于减少量化常数的内存占用。

具体来说，双重量化将第一次量化的量化常数 $c_2^{FP32}$ 作为输入进行第二次量化。第二步产生量化常数 $c_2^{FP8}$ 和第二层次的量化常数 $c_1^{FP32}$。我们使用块大小为64的8位浮点数进行第二次量化，在8位量化中没有性能下降。

对于块大小为64的情况下，双重量化将每个参数的内存占用从32/64 = 0.5 位减少到8/64 + 32/(64 * 256) = 0.127 位，即每个参数减少0.373 位。


(c) 分页优化器，用于管理内存峰值。

分页优化器利用 NVIDIA 的统一内存功能，它通过在 CPU 和 GPU 之间进行页面到页面的传输，避免了在 GPU 内存不足时的错误处理。该功能类似于 CPU 和磁盘之间的常规内存分页，但用于优化器状态。当 GPU 内存不足时，优化器状态会自动被分页到 CPU 的 RAM 中，并在需要时分页回 GPU。

# Learning to Prompt for Continual Learning
## 主要贡献
1. 提出L2P，一个基于提示持续学习的新型持续学习框架，通过学习提示池记忆空间来解决持续学习的挑战，这些提示作为参数化的 "指令"，供预训练的模型按顺序学习任务。该方法适用于处理最具挑战性的任务无关的持续学习。

2. 实验证明了L2P在多个持续学习基准上的有效性。

3. 在持续学习领域引入提示思想。

## 前期研究的问题
### Rehearsal-based methods
过于依赖缓冲区
### Architecture-based methods
任务身份未知时需要大量参数进行推断

## L2P的特点
### 提示池
**学习一个在任务相似时能够共享知识的模型，而在其他情况下保持知识的独立性**。因此，我们使用一个提示池来存储编码的知识，这些知识可以被灵活地组合作为模型的输入。提示池被定义为

$P = \{P_1, P_2, \dots, P_M\}, M = \text{total of prompts}$

其中 $P_j \in \mathbb{R}^{L_p \times D}$ 是一个具有标记长度 $L_p$ 和与 $x_e$ 相同的嵌入大小 $D$ 的单一提示。$x$ 和 $x_e = f_e(x)$ 分别为输入和其相应的嵌入特征。注意，在我们的符号中，我们省略了 $x$ 的任务索引 $t$，因为我们的方法对于任务无关的设置是足够通用的。用 $\{s_i\}_{i=1}^{N}$ 表示来自 $[1, M]$ 的 $N$ 个指数的子集，然后我们可以将输入嵌入进行如下调整。

$x_p = [P_{s_1}, \dots, P_{s_N}; x_e], 1 \leq N \leq M$

其中，`;` 代表标记token长度维度的连接。提示语句可以自由组合，因此它们可以**编码知识**（例如知识特征或任务信号）供模型处理。理想情况下，我们希望通过灵活的提示组合实现更精细的知识共享：类似的输入会在共享更多的共同提示，反之亦然。

### 实例级的查询策略
设计了一个基于键值对的查询策略，为不同的输入动态地选择合适的提示。

我们将每个提示值与一个可学习的键相关联：$\{(k_1, P_1), (k_2, P_2), \dots, (k_M, P_M)\}$，其中 $k_i \in \mathbb{R}^D$。而我们用 $K = \{k_i\}_{i=1}^M$ 表示所有键的集合。理想情况下，我们希望输入 $x$ 实例本身通过查询-键匹配来决定选择哪些提示。为此，我们引入一个查询函数 $q : \mathbb{R}^{H \times W \times C} \to \mathbb{R}^D$，将输入 $x$ 的编码为与键相同的维度。此外，对于不同的任务，$q$ 应该是一个确定性的函数，并且没有可学习的参数。我们假设使用预训练模型作为该查询的**特征提取**器来获得高阶特征 $q(x) = f_\theta(x)[0,:]$（我们用来对应于 [class] 的特征向量）。

我们用 $\gamma : \mathbb{R}^D \times \mathbb{R}^D \to \mathbb{R}$ 是一个对查询和提示键之间的匹配进行评分的函数。给定一个输入 $x$，我们用 $q(x)$ 通过简单的求解目标来查找前N个键。

$
K_x = \arg\min_{\{s_i\}_{i=1}^N \subseteq [1, M]} \sum_{i=1}^{N} \gamma(q(x), k_{s_i}), \text{equation}(3)
$

这种键值策略的设计使查询机制的学习和提示的学习过程脱钩，这一点已经被实验证明是至关重要的。此外，查询提示是以实例的方式进行的，这使得提示框架具有任务无关性，意味着该方法在训练结束时不需要明确的任务边界，在测试时也不需要任务身份。

可选择多样化的提示示例。虽然我们的方法不需要任务边界信息，但在现实世界的场景和实验数据**集中**，任务身份是离散的，所以任务边界训练也是已知的，这是非常普通的。我们发现，在我们的方法中加入这种提示选择，可以帮助模型学习更好的特定任务提示，尤其是在任务具有高度多样性的情况下。为此，我们提出了一个简单的扩展，**增加任务身份实验**，这对L2P来说是可选的。

在任务 t 的训练过程中，我们维护一个提示调速率 $H_t = [h_1, h_2, \dots, h_m]$，其中每个项 $h_i$ 代表到任务 t 的几项上。提示 $P_i$ 被选择的标记化加权输出。测试时，为了鼓励查询机制选择不同的提示，我们将公式修改为

$
K_x = \arg\min_{\{s_i\}_{i=1}^N \subseteq [1, M]} \sum_{i=1}^{N} \gamma(q(x), k_{s_i}) \cdot h_{s_i}
$
其中 $h_{s_i}$对被选中的经常使用的提示语进行惩罚，以鼓励多样化的选择。


