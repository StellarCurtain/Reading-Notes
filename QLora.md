# QLORA: Efficient Finetuning of Quantized LLMs
## 0 Abstract
我们提出了QLORA，这是一种高效的微调方法，它可以减少内存使用，使得在单个48GB GPU上微调一个具有650亿参数的模型成为可能，同时保留完整的16位微调任务性能。QLORA通过一个冻结的、4位量化的预训练语言模型，将梯度反向传播到低秩适配器（LoRA）中。我们最好的模型家族，命名为Guanaco，在Vicuna基准测试中表现优于所有之前公开发布的模型，达到ChatGPT性能水平的99.3%，并且只需要在单个GPU上进行24小时的微调。

QLORA引入了多项创新来节省内存而不牺牲性能：(a) 4位NormalFloat (NF4)，一种对于正态分布权重信息理论上最优的数据类型；(b) 双重量化，通过量化量化常数来减少平均内存占用；(c) 分页优化器，用于管理内存峰值。我们使用QLORA微调了超过1000个模型，提供了对8个指令数据集、多种模型类型（LLaMA、T5）和规模（如33B和65B参数模型）进行指令跟随和聊天机器人性能的详细分析，这在常规微调下是不可行的。我们的结果表明，QLORA在一个小型高质量数据集上进行微调，即使使用比之前最先进模型更小的模型，也能达到最先进的结果。我们提供了基于人类和GPT-4评估的聊天机器人性能的详细分析，显示GPT-4评估是一种便宜且合理的替代人类评估的方法。此外，我们发现当前的聊天机器人基准测试不足以准确评估聊天机器人的性能水平。通过一项“精心挑选”的分析，我们展示了Guanaco相比ChatGPT的不足之处。我们发布了所有模型和代码，包括用于4位训练的CUDA内核。

## 1 引言
微调大型语言模型（LLMs）是提高其性能并添加或移除不良行为的有效方法。然而，微调非常大的模型成本非常高；例如，微调一个具有650亿参数的LLaMA模型在常规16位情况下需要超过780GB的GPU内存。现有的量化方法可以减少LLM的内存占用，但这些技术仅在推理时有效，并且在训练过程中失效。

QLORA将预训练模型量化为4位，然后添加一小组可学习的低秩适配器权重，这些权重通过量化的权重反向传播梯度进行调优，显著减少内存需求，同时保持性能。

QLORA将微调一个650亿参数模型的内存需求从超过780GB减少到不到48GB，而不会降低运行时间或预测性能。这标志着LLM微调的可访问性发生了重大变化：现在最大的公开可用模型可以在单个GPU上进行微调。

QLORA引入了多个旨在减少内存使用但不牺牲性能的创新：(1) 4位NormalFloat，一种信息理论上最优的量化数据类型，适用于正态分布的数据，产生了比4位整数和4位浮点更好的经验结果。(2) 双重量化，这种方法量化了量化常数，节省了大约0.37位/参数的内存（对于650亿参数模型约节省3GB）。(3) 分页优化器，使用NVIDIA统一内存避免因梯度检查点处理迷你批次时产生的内存峰值。我们将这些贡献结合到一个改进的LoRA方法中，该方法在每一层网络中引入适配器，并避免了大多数以往工作中的准确性折衷。

QLORA的高效性使我们能够在内存开销极低的情况下微调多个大型语言模型，模型规模从80M到650亿参数。我们展示了QLORA不仅可以恢复16位性能，还能训练出先进的聊天机器人Guanaco。研究表明，数据质量比数据集规模更重要，例如，一个9K样本的数据集在聊天机器人性能上超过了一个450K样本的数据集。此外，我们发现多任务语言理解基准测试结果与聊天机器人性能并不完全相关。

我们还对聊天机器人的性能进行了基于人类和GPT-4的评估，使用了锦标赛式的基准测试。结果显示，GPT-4与人类评估的排名大致一致，但也存在一些分歧。基于模型的评估提供了一种成本较低的替代方案，但存在不确定性。

## 2 背景
### 块级k位量化
量化是将输入从具有更多信息的表示离散化为具有较少信息的表示的过程。它通常意味着将数据类型减少为更少的位数，例如将32位浮点数转换为8位整数。为了确保低位数据类型的整个范围都被使用，输入数据类型通常通过归一化到目标数据范围来重新缩放。例如，将32位浮点（FP32）张量量化为范围为 [−127,127]的Int8张量
$X^{\text{Int8}} = \text{round}\left( \frac{127}{\text{absmax}(X^{\text{FP32}})} \times X^{\text{FP32}} \right) = \text{round}(c^{\text{FP32}} \cdot X^{\text{FP32}}),$ 𝑐是量化常数或量化尺度。

反量化公式:$\text{dequant}(c^{\text{FP32}}, X^{\text{Int8}}) = \frac{X^{\text{Int8}}}{c^{\text{FP32}}} = X^{\text{FP32}}$

这个方法的问题在于，如果输入张量中出现较大数值（即离群值），那么量化区间中的某些位组合可能不会被有效利用，导致一些数值被量化到很少的区间中。为了解决这个离群值问题，通常的方法是将输入张量划分为独立的块，每个块都有自己的量化常数c。这可以形式化为：我们将输入张量 
$X∈R^{b×h}$切割为大小为B的连续块，通过展平输入张量并将线性段分割为 
$n= (b×h)/B​$个块。我们独立地量化这些块，并使用公式1来创建量化的张量和 
n个量化常数$c_i$。

### 低秩适配器

低秩适配器（LoRA）微调是一种通过使用一小组可训练参数（通常称为适配器）来减少内存需求的方法，同时保持完整模型参数固定。在随机梯度下降过程中，梯度通过冻结的预训练模型权重传递到适配器上，这些适配器被更新以优化损失函数。LoRA通过附加一个额外的因式分解投影来扩展线性投影。给定一个投影 $XW = Y$ 其中 $X \in \mathbb{R}^{b \times k}$，$W \in \mathbb{R}^{k \times o}$，LoRA计算：
$Y = XW + s \cdot X L_1 L_2,$

其中 $L_1 \in \mathbb{R}^{k \times r}$ 和 $L_2 \in \mathbb{R}^{r \times o}$，$s$ 为标量。

### 参数高效微调（PEFT）

参数高效微调方法的一个重要讨论点是，在微调LLM时，大部分内存占用来自于激活梯度，而非LoRA参数。例如，一个7B的LLaMA模型使用FLAN v2进行训练，批量大小为1，LoRA权重相当于常用的0.2%模型原始权重时，LoRA输入梯度的内存占用为567MB，而LoRA参数仅占用26MB。在使用梯度检查点后，输入梯度的内存占用降低到每个序列平均18MB，使得其内存占用甚至比所有LoRA权重加起来还要多。相比之下，4位量化基础模型消耗了5,048MB内存。

这表明，梯度检查点非常重要，但仅仅减少LoRA参数的内存占用带来的好处有限。这意味着我们可以使用更多的适配器，而不会显著增加整体训练的内存占用（具体细节见附录G）。如后续讨论所述，这对于恢复完整16位精度性能至关重要。

## 3 QLoRA 微调

QLoRA 通过我们提出的两种技术——4位 NormalFloat（NF4）量化和双重量化，达到了高精度的 4 位微调。此外，我们还引入了分页优化器，以防止在梯度检查点过程中因内存溢出错误而导致的大模型训练困难，这些错误通常使得单机微调困难。

QLoRA 使用一种低精度的存储数据类型，通常是4位，以及一种计算数据类型，通常是BFloat16。实际上，这意味着每当使用 QLoRA 权重张量时，我们将该张量反量化为 BFloat16，然后执行 16 位矩阵乘法。

我们现在讨论 QLoRA 的组成部分，接着给出 QLoRA 的正式定义。

### 4位 NormalFloat 量化

NormalFloat (NF) 数据类型基于分位数量化构建，这是一种信息理论上最优的数据类型，确保了每个量化区间从输入张量中分配了相同数量的值。分位数量化通过经验累积分布函数估计输入张量的分位数。

分位数量化的主要限制在于其计算代价昂贵。因此，使用近似分位数算法，如 SRAM 分位数，来进行量化估计。由于这些算法的近似特性，数据类型对离群值的量化误差较大，而这些离群值通常是最重要的数值。

当输入张量来自固定量化常数的分布时，可以避免昂贵的分位数估计和近似误差。在这种情况下，输入张量具有与量化常数相同的分位数，使得精确分位数估计在计算上可行。

由于预训练神经网络的权重通常具有零均值和标准差 $\sigma$ 的正态分布（见附录F），我们可以通过归一化将所有权重转化为具有单一固定分布，并缩放 $\sigma$ 使得它们精确匹配我们数据类型的范围。对于我们数据类型，我们将区间设定为$[-1, 1]$。因此，数据类型和神经网络权重的分位数都需要归一化到这个范围。

信息理论上最优的数据类型用于零均值正态分布，标准差 $\sigma$ 在$[-1, 1]$ 范围内。具体步骤如下：(1) 估计理论 $N(0,1)$ 分布的 $2^k + 1$ 分位数，以获得 k 位分位数量化数据类型；(2) 取这些数据并将其值归一化到$[-1, 1]$ 区间内；(3) 通过最大绝对值缩放，将输入权重张量量化到$[-1, 1]$ 区间内。

一旦权重范围和数据类型范围匹配，我们就可以像往常一样进行量化。步骤(3) 等同于重新缩放权重张量的标准偏差以匹配 k 位数据类型的标准偏差。更正式地，我们估计数据类型的 $2^k$ 值 $q_i$ 如下：

$q_i = \frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right) + Q_X\left(\frac{i+1}{2^k+1}\right)\right),$

其中，$Q_X$ 是标准正态分布 $N(0,1)$ 的分位数函数。对于对称的 k 位量化，由于该方法没有零的精确表示，因此重要的是确保准确填充零值和其他零值相关的操作不出错。为了确保0值点和使用所有 $2^k$ 位数据，我们通过估计两个范围 $q_i$ 来创建不对称的数据类型：负部分为 $2^k - 1$，正部分为 $2^k - 1 + 1$，并删除一对不会发生的值。

我们称这种数据类型为 **k 位 NormalFloat (NFk)**，因为该数据类型对零均值正态分布的数据在信息论上是最优的。该数据类型的确切值可以在附录E中找到。

### 双重量化

我们引入了 **双重量化 (Double Quantization, DQ)**，即量化量化常数，以进一步节省内存。虽然精确的 4 位量化需要较小的块大小，但它也带来了相当大的内存开销。例如，使用32位常数和64大小的块时，量化常数平均每个参数会增加0.5位。双重量化有助于减少量化常数的内存占用。

具体来说，双重量化将第一次量化的量化常数 $c_2^{FP32}$ 作为输入进行第二次量化。第二步产生量化常数 $c_2^{FP8}$ 和第二层次的量化常数 $c_1^{FP32}$。我们使用块大小为64的8位浮点数进行第二次量化，在8位量化中没有性能下降。

与 Dettmers 和 Zettlemoyer的结果一致，由于 $c_2^{FP32}$ 是正值，我们在量化前从 $c_2$ 中减去均值，并使用对称量化。对于块大小为64的情况下，双重量化将每个参数的内存占用从32/64 = 0.5 位减少到8/64 + 32/(64 * 256) = 0.127 位，即每个参数减少0.373 位。

### 分页优化器

分页优化器利用 NVIDIA 的统一内存功能，它通过在 CPU 和 GPU 之间进行页面到页面的传输，避免了在 GPU 内存不足时的错误处理。该功能类似于 CPU 和磁盘之间的常规内存分页，但用于优化器状态。当 GPU 内存不足时，优化器状态会自动被分页到 CPU 的 RAM 中，并在需要时分页回 GPU。

### QLoRA

使用上述描述的组件，我们定义了 QLoRA 在量化的基础模型中使用单个 LoRA 适配器的单层线性模型如下：

$Y^{\text{BF16}} = X^{\text{BF16}} \text{doubleDequant}(c_1^{\text{FP32}}, c_2^{k\text{-bit}}, W^{\text{NF4}}) + X^{\text{BF16}} L_1^{\text{BF16}} L_2^{\text{BF16}}$

其中，doubleDequant(·) 定义为：

$\text{doubleDequant}(c_1^{\text{FP32}}, c_2^{k\text{-bit}}, W^{\text{4bit}}) = W^{\text{BF16}}$

我们对 $W $ 使用 NF4 对 $c_2$ 使用 FP8。我们使用块大小为 64 的 $W $ 以获得更高的量化精度，并使用块大小为 256 的 $c_2$ 来节省内存。

对于参数更新，仅需要计算与适配器权重 $\frac{\partial E}{\partial L_i} $ 相关的梯度，而不涉及 4 位权重 $\frac{\partial E}{\partial W}$。然而，计算 $\frac{\partial E}{\partial L_i}$ 涉及通过公式（5）和通过 $W^{\text{NF4}}$ 的反量化到计算数据类型 $W^{\text{BF16}}$ 进行权重的导数计算 $\frac{\partial X}{\partial W}$，从而在 BFloat16 精度下计算导数。

总结，QLoRA 有一种存储数据类型（通常是4位 NormalFloat）和一种计算数据类型（16位 BrainFloat）。我们将存储数据类型反量化为计算数据类型以执行前向和后向传播，但我们只计算 LoRA 参数的权重梯度，这些参数使用16位 BrainFloat。

## 4 QLoRA vs. Standard Finetuning
我们已经讨论了QLoRA的工作原理，以及它如何显著减少微调模型所需的内存。现在的主要问题是QLoRA的性能是否能与全模型微调一样出色。此外，我们还希望分析QLoRA的组成部分，包括NormalFloat4相较于标准Float4的影响。接下来的部分将讨论旨在回答这些问题的实验。

### 实验设置
我们考虑了三种架构（编码器、编码器-解码器和仅解码器），并将 QLoRA 与 16 位适配器微调和全微调的模型（最大至 3B 参数）进行了比较。

尽管分页优化器对于在单个 24GB/48GB GPU 上进行 33B/65B QLoRA 微调至关重要，但我们没有提供分页优化器的硬件测量，因为分页只在处理长序列迷你批次时才会发生，这种情况较少见。然而，我们对分页优化器在 48GB GPU 上的 65B 模型的运行情况进行了分析，发现使用 16 批量大小时，分页优化器提供的训练速度与常规优化器相同。未来的工作应测量并确定分页过程中发生慢速情况的具体情境。

### 默认的 LoRA 超参数无法匹配 16 位性能
当使用标准的做法将 LoRA 应用于查询和值注意力投影矩阵时【28】，我们无法复制大型基础模型的全微调性能。如图 2 所示，在 LLaMA 7B 模型上对 Alpaca 进行微调时，我们发现最关键的 LoRA 超参数是总共使用多少个 LoRA 适配器，以及在所有线性 Transformer 块层上使用 LoRA 是匹配全微调性能的必要条件。其他 LoRA 超参数，如投影维度 r，不会影响性能（见附录 A）。

### 4 位 NormalFloat 比 4 位浮点表现更好
虽然 4 位 NormalFloat (NF4) 数据类型在信息理论上最优，但仍需确定这一特性是否转化为实际优势。我们在不同大小的模型（125M 至 65B）上评估了量化的 LLM进行的语言建模和一组零次任务。在图 3 和表 2 中，我们看到 NF4 在零次精度任务上的表现显著优于 FP4 和 Int4，并且双重量化进一步减少了内存占用。
